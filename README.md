# ğŸ—ºï¸ Google Maps Scraper API

A **high-performance FastAPI service** for scraping Google Maps data with **parallel processing** and **comprehensive data extraction**. Ideal for n8n users and automation workflows.

[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=flat-square&logo=docker&logoColor=white)](https://www.docker.com/)
[![Python](https://img.shields.io/badge/python-3.10-blue.svg?style=flat-square&logo=python&logoColor=white)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=flat-square&logo=fastapi)](https://fastapi.tiangolo.com/)

---

## âœ¨ Key Features

- âš¡ **3x faster** with parallel processing (3 workers)
- ğŸ“Š **Two extraction modes**: Quick list or complete details
- ğŸ“ **Full contact info**: Phone, website, address, opening hours
- ğŸ›¡ï¸ **Anti-detection**: Random delays, realistic user-agent, WebDriver masking
- ğŸ³ **Docker optimized**: shm_size, single-process mode for stability
- ğŸŒ **Multi-language support**: en, fr, es, de, and more
- ğŸ”„ **n8n compatible**: Ready-to-use with automation workflows

---

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- 2 GB RAM minimum
- Port 8001 available

### Installation

```bash
# Clone the repository
git clone https://github.com/conor-is-my-name/google-maps-scraper.git
cd google-maps-scraper

# Start with Docker
docker compose up -d

# Test the API
curl "http://localhost:8001/health"
```

Expected response:
```json
{
  "status": "healthy",
  "service": "google-maps-scraper"
}
```

---

## ğŸ“– API Usage

### Endpoints

- **GET `/scrape-get`** - Main endpoint for scraping (recommended for n8n)
- **POST `/scrape`** - Alternative POST endpoint with JSON body
- **GET `/health`** - Health check endpoint
- **GET `/`** - Service information

### Parameters

| Parameter | Type | Default | Required | Description |
|-----------|------|---------|----------|-------------|
| `query` | string | - | âœ… | Search query (e.g., "hotels in 98392") |
| `max_places` | int | 10 | âŒ | Maximum results (1-100) |
| `lang` | string | "en" | âŒ | Language code (en, fr, es, de, etc.) |
| `headless` | bool | true | âŒ | Run browser in headless mode |
| **`details`** | **bool** | **false** | âŒ | **Extract full details (phone, website, etc.)** |

---

## ğŸ¯ Extraction Modes

### Quick Mode (`details=false`)

Fast extraction of basic information - **~2 seconds per place**

**Returns:**
- âœ… Name
- âœ… URL
- âœ… Rating
- âœ… Review count
- âœ… Category

```bash
curl "http://localhost:8001/scrape-get?query=restaurant%20paris&max_places=20&details=false"
```

**Use cases:** Rankings, comparisons, quick lists

---

### Full Details Mode (`details=true`)

Complete extraction including contact information - **~2-3 seconds per place**

**Returns:**
- âœ… Name, URL, Rating, Review count, Category
- âœ… **Phone number**
- âœ… **Website**
- âœ… **Full address**
- âœ… **Opening hours**

```bash
curl "http://localhost:8001/scrape-get?query=restaurant%20paris&max_places=10&details=true"
```

**Use cases:** Directory creation, CRM import, contact lists

---

## ğŸ“Š Performance

| Places | Quick Mode | Full Details | Sequential (old) |
|--------|------------|--------------|------------------|
| 5 | ~10s | ~15s | ~45s |
| 10 | ~20s | ~30s | ~90s |
| 20 | ~40s | ~60s | ~180s |

**Speedup:** 2-3x faster than sequential scraping thanks to 3 parallel workers.

---

## ğŸ’¡ Example Requests

### Quick List (Basic Info)

```bash
# Get 20 restaurants with ratings
curl "http://localhost:8001/scrape-get?query=restaurant%20paris&max_places=20&details=false&lang=en"
```

**Response:**
```json
{
  "success": true,
  "query": "restaurant paris",
  "total_results": 20,
  "results": [
    {
      "name": "Le Meurice",
      "url": "https://www.google.com/maps/place/Le+Meurice/...",
      "rating": "4.8",
      "reviews_count": "1234",
      "category": "French restaurant"
    }
  ]
}
```

---

### Full Details (Complete Info)

```bash
# Get 10 restaurants with contact details
curl "http://localhost:8001/scrape-get?query=restaurant%20paris&max_places=10&details=true&lang=fr"
```

**Response:**
```json
{
  "success": true,
  "query": "restaurant paris",
  "total_results": 10,
  "results": [
    {
      "name": "Le Meurice",
      "url": "https://www.google.com/maps/place/Le+Meurice/...",
      "rating": "4.8",
      "reviews_count": "1234",
      "category": "French restaurant",
      "phone": "+33 1 44 58 10 10",
      "website": "https://www.dorchestercollection.com/paris/le-meurice",
      "address": "228 Rue de Rivoli, 75001 Paris",
      "hours": "Open Â· Closes 10:30 pm"
    }
  ]
}
```

---

## ğŸ”— n8n Integration

### HTTP Request Node Configuration

```
Method: GET
URL: http://gmaps_scraper_api_service:8001/scrape-get

Query Parameters:
  - query: {{ $json.search_query }}
  - max_places: 20
  - details: true
  - lang: en
  - headless: true
```

### Workflow Example

1. **Trigger** â†’ Schedule or Webhook
2. **HTTP Request** â†’ Google Maps Scraper
3. **Code** â†’ Parse and filter results
4. **Database** â†’ Store in PostgreSQL/MySQL
5. **Notification** â†’ Send to Slack/Email

**Designed for:** [n8n-autoscaling](https://github.com/conor-is-my-name/n8n-autoscaling)

---

## ğŸ‹ Docker Commands

```bash
# Start service
docker compose up -d

# View logs
docker compose logs -f

# Stop service
docker compose down

# Rebuild after changes
docker compose down
docker compose build --no-cache
docker compose up -d

# Quick restart
docker compose restart
```

---

## âš™ï¸ Configuration

### Docker Compose Settings

**docker-compose.yml** includes optimizations for stability:

```yaml
services:
  gmaps_scraper_api_service:
    shm_size: '2gb'          # Prevents "Page crashed" errors
    environment:
      - DISPLAY=:99          # Xvfb display
      - PYTHONUNBUFFERED=1
    volumes:
      - /dev/shm:/dev/shm    # Shared memory for Chromium
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
```

### Modify Worker Count

To adjust parallel processing (default: 3 workers):

Edit `gmaps_scraper_server/scraper.py` line ~217:

```python
max_workers=3  # Increase to 5 max (higher = risk of detection)
```

âš ï¸ **Warning:** More than 5 workers may trigger Google's bot detection.

---

## ğŸ›¡ï¸ Anti-Detection Features

The scraper includes multiple protections:

- âœ… **Realistic User-Agent** (Chrome 120)
- âœ… **WebDriver masking** (`navigator.webdriver = undefined`)
- âœ… **Random delays** between actions (1-3 seconds)
- âœ… **Chromium stealth arguments** (`--disable-blink-features=AutomationControlled`)
- âœ… **Limited concurrency** (3 workers max for normal behavior)

**Recommended limits:**
- Max 500 places/day per IP
- Min 60 seconds between requests
- Max 3-5 parallel workers

---

## ğŸ”§ Local Development

### Without Docker

```bash
# Install dependencies
pip install -r requirements.txt

# Install Playwright browsers
playwright install chromium

# Run the API
uvicorn gmaps_scraper_server.main_api:app --reload --host 0.0.0.0 --port 8001
```

The API will be available at `http://localhost:8001`

### With Docker (recommended)

```bash
docker compose up --build
```

---

## ğŸ“ Project Structure

```
google-maps-scraper/
â”œâ”€â”€ docker-compose.yml          # Docker configuration
â”œâ”€â”€ Dockerfile                  # Docker image build
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ start.sh                    # Container startup script
â”œâ”€â”€ gmaps_scraper_server/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main_api.py            # FastAPI endpoints
â”‚   â””â”€â”€ scraper.py             # Scraping logic (parallel)
â””â”€â”€ debug_screenshots/          # Debug screenshots
```

---

## ğŸ› Troubleshooting

### "Page crashed" Error

**Cause:** Insufficient shared memory

**Solution:**
```yaml
# In docker-compose.yml
shm_size: '2gb'  # Increase to 4gb if needed
```

### No Results Returned

**Possible causes:**
1. Google structure changed â†’ Update CSS selectors
2. CAPTCHA detected â†’ Reduce workers, wait 24h
3. Invalid query â†’ Check URL encoding

### View Logs

```bash
docker compose logs -f
```

---

## âš ï¸ Important Notes

- âš ï¸ **Rate limiting:** Respect Google's terms. Max 500 places/day recommended.
- âš ï¸ **Legal compliance:** Check local laws regarding web scraping.
- âš ï¸ **Responsible use:** This tool is for educational and legitimate business purposes.
- âš ï¸ **No guarantees:** Google may change their structure at any time.

---

## ğŸ“ˆ Changelog

### v2.0.0 (Latest)
- âœ¨ Added parallel processing (3 workers) - 3x faster
- âœ¨ Added full details mode (phone, website, address, hours)
- âœ¨ Added random delays for anti-detection
- ğŸ”§ Fixed "Page crashed" with `--single-process`
- ğŸ”§ Improved data extraction reliability
- ğŸ“š Complete documentation and examples

### v1.0.0
- ğŸ‰ Initial release with basic scraping

---

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

MIT License - Free to use, modify, and distribute.

---

## ğŸ™ Credits

Original project structure by [@conor-is-my-name](https://github.com/conor-is-my-name)

Enhancements:
- Parallel processing implementation
- Full details extraction mode
- Anti-detection improvements
- Docker optimizations
- Comprehensive documentation

---

## ğŸ“§ Support

- ğŸ› **Issues:** [GitHub Issues](https://github.com/conor-is-my-name/google-maps-scraper/issues)
- ğŸ’¬ **Discussions:** [GitHub Discussions](https://github.com/conor-is-my-name/google-maps-scraper/discussions)

---

**Built with â¤ï¸ using FastAPI and Playwright**

